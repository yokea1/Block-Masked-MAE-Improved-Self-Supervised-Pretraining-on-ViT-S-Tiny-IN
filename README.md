# Block-Masked-MAE-Improved-Self-Supervised-Pretraining-on-ViT-S-Tiny-IN

**Tiny-ImageNet â€¢ Vision Transformer â€¢ Masked Autoencoding â€¢ Texture Bias Reduction**

This repository contains the official implementation of **Block-Masked MAE**, a structure-oriented variant of Masked Autoencoders for Vision Transformers.  
Instead of random patch masking, we introduce **contiguous 2Ã—2 / 4Ã—4 block masking** to reduce texture shortcuts and force ViT to learn **semantic-level representations**.

---
## Highlights

- **Block-wise Masking (2Ã—2 / 4Ã—4)**  
  Encourages semantic reconstruction by removing fine-grained texture cues.

- **Semantic-vs-Texture Diagnostics**  
  Includes low-pass robustness, Stylized-ImageNet transfer, DTD texture dataset transfer, and occlusion-based attention rollout.

- **Long-run Pretraining (400â€“800 epochs)**  
  Full Tiny-ImageNet MAE training to test representation stability and semantic abstraction.

- **Reproducible Pipeline**  
  Config-driven, deterministic seeds, structured logs, automatic plotting, ablation modules, experiment registry.

- **Complete Ablation Suite**  
  Block size, mask ratio, patch size, decoder depth, positional embeddings.


---
## Project Structure

```bash
block-mae/
â”‚â”€â”€ configs/                
â”‚â”€â”€ data/                   
â”‚â”€â”€ models/
â”‚   â”œâ”€â”€ mae_vit_s.py        
â”‚   â””â”€â”€ mask_generator.py   
â”‚â”€â”€ train/
â”‚   â”œâ”€â”€ train_pretrain.py   
â”‚   â””â”€â”€ train_linear.py     
â”‚â”€â”€ eval/
â”‚   â”œâ”€â”€ lowpass.py          
â”‚   â”œâ”€â”€ stylized_in.py      
â”‚   â”œâ”€â”€ dtd_transfer.py     
â”‚   â””â”€â”€ attention_rollout.py
â”‚â”€â”€ utils/                  
â”‚â”€â”€ outputs/                
â””â”€â”€ README.md


---

##  Method Overview

### ğŸ”³ Block-wise Masking
Instead of independently sampling patch indices like MAE,  
we mask **contiguous spatial blocks**:

1Ã—1 (standard MAE)
2Ã—2 (ours)
4Ã—4 (ours)

Block masks introduce **structured ambiguity**, making texture-based shortcuts harder and forcing the model to infer **object shape**.

---

## ğŸ“ˆ Experiments

### âœ” Pretraining Setup
| Setting | Value |
|--------|-------|
| Model | ViT-S (MAE) |
| Dataset | Tiny-ImageNet (SSL) |
| Epochs | 20, 50, 200, 400, 800 |
| Mask ratios | 65%, 75%, 85% |
| Block sizes | 1Ã—1, 2Ã—2, 4Ã—4 |
| Optimizer | AdamW |
| Scheduler | Cosine Annealing |

---

## ğŸ¯ Downstream Evaluation (Linear Probe)
We evaluate on:

- Tiny-ImageNet
- CIFAR-10
- CIFAR-100

Metrics:
- Top-1 accuracy
- Top-5 accuracy

(Replace below once results ready)

To be updated after long-run training.

yaml
Copy code

---

## ğŸ§ª Ablations

### 1. Block Size
- 1Ã—1 (MAE baseline)
- 2Ã—2 (ours)
- 4Ã—4 (ours)

### 2. Mask Ratio
- 65 / 75 / 85%

### 3. Decoder Depth
- 1 / 2 / 4 / 8 layers

### 4. Patch Size
- 16Ã—16 vs 8Ã—8

### 5. Positional Embedding
- Absolute
- Relative
- None (learned)

---

## ğŸ” Semantic vs Texture Bias Analysis

### â‘  Low-pass Robustness  
Gaussian blur & FFT-based filtering to test shape sensitivity.

### â‘¡ Stylized ImageNet (SIN) Transfer  
Tests reliance on texture vs structure.

### â‘¢ DTD Texture Dataset Transfer  
Texture-biased models perform excessively well;  
block-masked MAE should drop more â†’ more semantic.

### â‘£ Occlusion-based Attention Rollout  
Stable attention = structure-based representation.

---

## ğŸ–¼ï¸ Visualizations

### Reconstruction Samples
- Random mask vs Block mask
- 20 / 50 / 200 / 400 / 800 epoch comparison

### Attention Maps
- Attention rollout under occlusion  
- Center-of-mass tracking  
- Entropy heatmaps

---

## ğŸ§© How to Run

### Pretraining (MAE)
```bash
python train/train_pretrain.py --config configs/pretrain_block.yaml
Linear Probing
bash
Copy code
python train/train_linear.py --config configs/linear_probe.yaml
Low-pass Robustness
bash
Copy code
python eval/lowpass.py --checkpoint <path>
ğŸ“¦ Dependencies
PyTorch >= 2.1

timm

numpy / scipy

matplotlib

einops

pyyaml

tqdm

Install:

bash
Copy code
pip install -r requirements.txt
ğŸ“„ Citation (Template)
bibtex
Copy code
@article{he2025blockmae,
  title={Block-Masked MAE: Structure-Oriented Self-Supervised Pretraining on Vision Transformers},
  author={He, Yuke},
  year={2025},
  note={Work in progress}
}
ğŸ—‚ï¸ Status
âœ…Block mask generator

âœ…20/50 epoch pilot experiments

âœ…Config-driven reproducible pipeline

 400â€“800 epoch long-run experiments

 Full semantic vs texture ablation

 Research draft (8â€“12 pages)

ğŸ“¨ Contact

He Yuke
GitHub: https://github.com/yokea1

Email: 217885@student.upm.edu.my
